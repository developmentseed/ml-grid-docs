---
title: Big data
date: 2012-08-23
layout: post.html
---

## Downloading large datasets
Perhaps the biggest challenge in this project was handling the tens of millions of images for each country. HV towers only become recognizable at about zoom 18 imagery (~0.5 meter/pixel resolution), which is a relatively high spatial resolution for commercially available satellite imagery. The cost associated with this high spatial resolution is that we were forced to handle a very large volume of tiles. Just the act of downloading the imagery from Mapbox's servers was extremely computationally intensive. Tens of thousands of networking coroutines, which are essentially lightweight threads each downloading a single image at a time, were run in parallel. This was the only method we found to obtain the country-wide image sets in a reasonable amount of time. It also required us to keep track of which files had been downloaded to avoid wasting time and resources; at nearly a hundred million image tiles, tracking this process is no longer trivial. Standard operating systems will throw an error if you simply try to list or delete a folder with this many files.

For the next iteration, we will likely use AWS's Simple Queue Service (SQS) to keep track of all tiles that need downloading. Each entry will contain tile indices, which our downloading script can pull from asynchronously. Then, each entry in the queue is only removed if our downloading process sends back confirmation that it worked correctly. This should prevent a fault tolerant and more efficient solution that could also scale if we needed to process more countries. Additionally, it will also allow us to more easily download a proportion of the total images. In many cases, the ML algorithm does not need to predict the entire countries image for the strings of HV towers to become visible to a human in an ML-generated map overlay. In the first iteration, the download script randomly skipped over images with a probability dependent on the desired download proportion (i.e., if we only wanted half of a country's images, we would set this skip probability to 0.5). Using SQS, we can randomly shuffle the queue once and then download the desired number of images directly instead of wasting computation in this skip procedure. 

## Titrating download and prediction speed
In this project, we also decided to download all imagery directly to AWS S3 for storage. This choice was useful in that we never had to request any imagery twice from Mapbox and we believed we could rapidly transfer images from S3 to EC2 instances for the inference stage, which was dependent on GPU instances that were relatively expensive to reserve. This essentially gave us a complete copy of all the data that we intended to process. During the inference stage, we then transferred data from S3 to the EC2 instances that were actually performing inference.

However, the storing and accessing data on S3 was both costly and slow. There are set financial costs associated with the raw volume of stored data as well as costs for each file uploaded or downloaded to/from S3. But accessing stored data was more expensive than expected simply because of the sheer number of files that we were required to process. Additionally, the speed at which we could access stored data was much slower than expected. This likely occurred for two reasons: First, since S3 accesses files using a key-based system, any request for a subset of imagery (using asterisk wildcards for example) required the AWS servers to iterate over all keys to find the relevant subset; again, because of the large volume of files, this was surprisingly slow â€” in some cases, an S3 copy request could take 1-2 hours before the download even started. Second, the effective download speed (in Mbps) was also quite slow. We found there is a fixed overhead computational cost to initiating the download of a single file. As an practical example, this means that downloading one-thousand 1kb files is much slower than downloading one 1 Mb file. Empirically, we found that transferring data from S3 to EC2 instances (for inference) was about 100x slower than AWS is capable of had the same data been stored as a single file.

Future efforts would be well spent to completely remove any processes involving S3 and instead finding methods of downloading imagery directly to EC2 instances immediately before prediction. The challenge here is to match the speed that images are downloaded with the speed of prediction. Currently, the download step is 2-5x slower depending on the GPU instance used for prediction. Download speed could be increased by finding a better method of requesting images from Mapbox (currently done of HTTP). It may also be possible to somehow construct super-images each made up of 25 or 100 individual tiles (in 5x5 or 10x10 squares, respectively) prior to downloading them. This would address some of those constant overhead and possibly reduce the effective per-image download time. Even something as simple as combining large groups of images into single zip files might help reduce the slowdowns we experienced. 
